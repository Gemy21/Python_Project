import sqlite3
import json
import os
from datetime import datetime
import shutil

class DataSync:
    def __init__(self, db_name="company_accounts.db"):
        self.db_name = db_name
        self.exports_folder = "data_exports"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if not os.path.exists(self.exports_folder):
            os.makedirs(self.exports_folder)
    
    def get_connection(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        return sqlite3.connect(self.db_name)
    
    def export_all_data(self, filename=None):
        """
        ØªØµØ¯ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„Ù JSON
        
        Args:
            filename: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù (Ø§Ø®ØªÙŠØ§Ø±ÙŠ). Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø­Ø§Ù„ÙŠ
        
        Returns:
            str: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙØµØ¯ÙÙ‘Ø±
        """
        if filename is None:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ ÙƒØ§Ø³Ù… Ù„Ù„Ù…Ù„Ù
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"backup_{timestamp}.json"
        
        filepath = os.path.join(self.exports_folder, filename)
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ØªÙŠ Ù†Ø±ÙŠØ¯ ØªØµØ¯ÙŠØ±Ù‡Ø§
        tables = [
            'sellers_accounts',
            'seller_transactions',
            'clients_accounts',
            'inventory_items',
            'meals',
            'agriculture_transfers',
            'expenses',
            'client_invoices'
        ]
        
        export_data = {
            'export_date': datetime.now().isoformat(),
            'database_name': self.db_name,
            'tables': {}
        }
        
        for table in tables:
            try:
                # Ø¬Ù„Ø¨ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
                cursor.execute(f"PRAGMA table_info({table})")
                columns_info = cursor.fetchall()
                columns = [col[1] for col in columns_info]
                
                # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                cursor.execute(f"SELECT * FROM {table}")
                rows = cursor.fetchall()
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
                table_data = []
                for row in rows:
                    row_dict = {}
                    for i, col in enumerate(columns):
                        row_dict[col] = row[i]
                    table_data.append(row_dict)
                
                export_data['tables'][table] = {
                    'columns': columns,
                    'data': table_data,
                    'row_count': len(table_data)
                }
                
                print(f"âœ“ ØªÙ… ØªØµØ¯ÙŠØ± {len(table_data)} Ø³Ø¬Ù„ Ù…Ù† Ø¬Ø¯ÙˆÙ„ {table}")
                
            except sqlite3.OperationalError as e:
                print(f"âš  ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ {table} - {e}")
                continue
        
        conn.close()
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù JSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰: {filepath}")
        print(f"Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {os.path.getsize(filepath) / 1024:.2f} KB")
        
        return filepath
    
    def import_data(self, filepath, merge_mode='update'):
        """
        Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù JSON Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        
        Args:
            filepath: Ù…Ø³Ø§Ø± Ù…Ù„Ù JSON
            merge_mode: Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¯Ù…Ø¬
                - 'replace': Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                - 'update': ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                - 'skip': ØªØ®Ø·ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø·
        
        Returns:
            dict: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {filepath}")
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù
        with open(filepath, 'r', encoding='utf-8') as f:
            import_data = json.load(f)
        
        print(f"ğŸ“¥ Ø¨Ø¯Ø¡ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†: {filepath}")
        print(f"ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØµØ¯ÙŠØ±: {import_data.get('export_date', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        stats = {
            'tables_processed': 0,
            'rows_inserted': 0,
            'rows_updated': 0,
            'rows_skipped': 0,
            'errors': []
        }
        
        for table_name, table_info in import_data['tables'].items():
            print(f"\nâš™ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ø¯ÙˆÙ„: {table_name}")
            
            columns = table_info['columns']
            data = table_info['data']
            
            # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù…ÙˆØ¯ id Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù„Ù„Ø¥Ø¯Ø±Ø§Ø¬ (Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹)
            insert_columns = [col for col in columns if col not in ['id', 'created_at', 'updated_at']]
            
            for row_dict in data:
                try:
                    if merge_mode == 'replace':
                        # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
                        if 'id' in row_dict and row_dict['id']:
                            cursor.execute(f"DELETE FROM {table_name} WHERE id = ?", (row_dict['id'],))
                        
                        # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
                        placeholders = ', '.join(['?' for _ in insert_columns])
                        columns_str = ', '.join(insert_columns)
                        values = [row_dict.get(col) for col in insert_columns]
                        
                        cursor.execute(
                            f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})",
                            values
                        )
                        stats['rows_inserted'] += 1
                    
                    elif merge_mode == 'update':
                        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø£ÙˆÙ„Ø§Ù‹
                        if 'id' in row_dict and row_dict['id']:
                            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø³Ø¬Ù„
                            cursor.execute(f"SELECT id FROM {table_name} WHERE id = ?", (row_dict['id'],))
                            exists = cursor.fetchone()
                            
                            if exists:
                                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
                                set_clause = ', '.join([f"{col} = ?" for col in insert_columns])
                                values = [row_dict.get(col) for col in insert_columns]
                                values.append(row_dict['id'])
                                
                                cursor.execute(
                                    f"UPDATE {table_name} SET {set_clause} WHERE id = ?",
                                    values
                                )
                                stats['rows_updated'] += 1
                            else:
                                # Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯
                                placeholders = ', '.join(['?' for _ in insert_columns])
                                columns_str = ', '.join(insert_columns)
                                values = [row_dict.get(col) for col in insert_columns]
                                
                                cursor.execute(
                                    f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})",
                                    values
                                )
                                stats['rows_inserted'] += 1
                        else:
                            # Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯ (Ø¨Ø¯ÙˆÙ† id)
                            placeholders = ', '.join(['?' for _ in insert_columns])
                            columns_str = ', '.join(insert_columns)
                            values = [row_dict.get(col) for col in insert_columns]
                            
                            cursor.execute(
                                f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})",
                                values
                            )
                            stats['rows_inserted'] += 1
                    
                    elif merge_mode == 'skip':
                        # Ø¥Ø¯Ø±Ø§Ø¬ ÙÙ‚Ø· Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³Ø¬Ù„ Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
                        if 'id' in row_dict and row_dict['id']:
                            cursor.execute(f"SELECT id FROM {table_name} WHERE id = ?", (row_dict['id'],))
                            exists = cursor.fetchone()
                            
                            if exists:
                                stats['rows_skipped'] += 1
                                continue
                        
                        # Ø¥Ø¯Ø±Ø§Ø¬ Ø³Ø¬Ù„ Ø¬Ø¯ÙŠØ¯
                        placeholders = ', '.join(['?' for _ in insert_columns])
                        columns_str = ', '.join(insert_columns)
                        values = [row_dict.get(col) for col in insert_columns]
                        
                        cursor.execute(
                            f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})",
                            values
                        )
                        stats['rows_inserted'] += 1
                
                except Exception as e:
                    error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ø¯ÙˆÙ„ {table_name}: {str(e)}"
                    stats['errors'].append(error_msg)
                    print(f"âš  {error_msg}")
            
            stats['tables_processed'] += 1
            print(f"âœ“ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ø¯ÙˆÙ„ {table_name}")
        
        conn.commit()
        conn.close()
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        print("\n" + "="*50)
        print("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯:")
        print(f"  â€¢ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {stats['tables_processed']}")
        print(f"  â€¢ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙØ¯Ø±Ø¬Ø©: {stats['rows_inserted']}")
        print(f"  â€¢ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙØ­Ø¯Ø«Ø©: {stats['rows_updated']}")
        print(f"  â€¢ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ØªØ®Ø·Ø§Ø©: {stats['rows_skipped']}")
        print(f"  â€¢ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {len(stats['errors'])}")
        print("="*50)
        
        return stats
    
    def create_daily_backup(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙŠÙˆÙ…ÙŠØ©"""
        today = datetime.now().strftime("%Y-%m-%d")
        filename = f"daily_backup_{today}.json"
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ…
        filepath = os.path.join(self.exports_folder, filename)
        if os.path.exists(filepath):
            print(f"âš  ØªÙˆØ¬Ø¯ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ… Ø¨Ø§Ù„ÙØ¹Ù„: {filepath}")
            # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø¨Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"backup_{timestamp}.json"
        
        return self.export_all_data(filename)
    
    def list_backups(self):
        """Ø¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        if not os.path.exists(self.exports_folder):
            print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
            return []
        
        backups = []
        for filename in os.listdir(self.exports_folder):
            if filename.endswith('.json'):
                filepath = os.path.join(self.exports_folder, filename)
                size = os.path.getsize(filepath) / 1024  # KB
                modified = datetime.fromtimestamp(os.path.getmtime(filepath))
                
                backups.append({
                    'filename': filename,
                    'filepath': filepath,
                    'size_kb': size,
                    'modified': modified
                })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹)
        backups.sort(key=lambda x: x['modified'], reverse=True)
        
        if backups:
            print("\nğŸ“¦ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:")
            print("-" * 80)
            for i, backup in enumerate(backups, 1):
                print(f"{i}. {backup['filename']}")
                print(f"   Ø§Ù„ØªØ§Ø±ÙŠØ®: {backup['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"   Ø§Ù„Ø­Ø¬Ù…: {backup['size_kb']:.2f} KB")
                print("-" * 80)
        else:
            print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
        
        return backups
    
    def cleanup_old_backups(self, keep_days=30):
        """
        Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        
        Args:
            keep_days: Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù… Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        """
        if not os.path.exists(self.exports_folder):
            return
        
        cutoff_date = datetime.now().timestamp() - (keep_days * 24 * 60 * 60)
        deleted_count = 0
        
        for filename in os.listdir(self.exports_folder):
            if filename.endswith('.json'):
                filepath = os.path.join(self.exports_folder, filename)
                if os.path.getmtime(filepath) < cutoff_date:
                    os.remove(filepath)
                    deleted_count += 1
                    print(f"ğŸ—‘ ØªÙ… Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: {filename}")
        
        if deleted_count > 0:
            print(f"\nâœ“ ØªÙ… Ø­Ø°Ù {deleted_count} Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©")
        else:
            print("âœ“ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù„Ø­Ø°Ù")


# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹
def quick_export():
    """ØªØµØ¯ÙŠØ± Ø³Ø±ÙŠØ¹ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    sync = DataSync()
    return sync.create_daily_backup()

def quick_import(filepath, merge_mode='update'):
    """Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø³Ø±ÙŠØ¹ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    sync = DataSync()
    return sync.import_data(filepath, merge_mode)


if __name__ == "__main__":
    # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    print("=" * 60)
    print("Ù†Ø¸Ø§Ù… Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    print("=" * 60)
    
    sync = DataSync()
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
    while True:
        print("\n" + "=" * 60)
        print("Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…Ù„ÙŠØ©:")
        print("1. ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©)")
        print("2. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        print("3. Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
        print("4. Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©")
        print("5. Ø®Ø±ÙˆØ¬")
        print("=" * 60)
        
        choice = input("\nØ§Ø®ØªÙŠØ§Ø±Ùƒ: ").strip()
        
        if choice == '1':
            print("\nğŸ“¤ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            filepath = sync.create_daily_backup()
            print(f"\nâœ“ ØªÙ… Ø§Ù„ØªØµØ¯ÙŠØ± Ø¨Ù†Ø¬Ø§Ø­!")
            
        elif choice == '2':
            backups = sync.list_backups()
            if not backups:
                print("\nâš  Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯")
                continue
            
            print("\nØ§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ (Ø£Ùˆ 0 Ù„Ù„Ø¥Ù„ØºØ§Ø¡):")
            try:
                backup_num = int(input("Ø§Ù„Ø±Ù‚Ù…: ").strip())
                if backup_num == 0:
                    continue
                if 1 <= backup_num <= len(backups):
                    filepath = backups[backup_num - 1]['filepath']
                    
                    print("\nØ§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¯Ù…Ø¬:")
                    print("1. ØªØ­Ø¯ÙŠØ« (update) - ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©")
                    print("2. Ø§Ø³ØªØ¨Ø¯Ø§Ù„ (replace) - Ø­Ø°Ù Ø§Ù„Ù‚Ø¯ÙŠÙ… ÙˆØ§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ø§Ù„Ø¬Ø¯ÙŠØ¯")
                    print("3. ØªØ®Ø·ÙŠ (skip) - Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙ‚Ø· ÙˆØªØ®Ø·ÙŠ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯")
                    
                    merge_choice = input("\nØ§Ø®ØªÙŠØ§Ø±Ùƒ (1/2/3): ").strip()
                    merge_modes = {'1': 'update', '2': 'replace', '3': 'skip'}
                    merge_mode = merge_modes.get(merge_choice, 'update')
                    
                    print(f"\nğŸ“¥ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© {merge_mode}...")
                    stats = sync.import_data(filepath, merge_mode)
                    print("\nâœ“ ØªÙ… Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¨Ù†Ø¬Ø§Ø­!")
                else:
                    print("âš  Ø±Ù‚Ù… ØºÙŠØ± ØµØ­ÙŠØ­")
            except ValueError:
                print("âš  ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù… ØµØ­ÙŠØ­")
            
        elif choice == '3':
            sync.list_backups()
            
        elif choice == '4':
            try:
                days = int(input("\nØ¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù… Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù†Ø³Ø® (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 30): ").strip() or "30")
                sync.cleanup_old_backups(days)
            except ValueError:
                print("âš  ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù… ØµØ­ÙŠØ­")
            
        elif choice == '5':
            print("\nğŸ‘‹ Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        
        else:
            print("\nâš  Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­")
